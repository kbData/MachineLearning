---
title: "Report"
author: "Kiryl Batsiukov"
date: "03/20/2015"
output: html_document
---

This document describes the machine learning algorithm developed to predict the manner in which the exercise was performed by different pearsons under test. The dataset is described under <http://groupware.les.inf.puc-rio.br/har>.

We download the dataset form the internet and read the downloaded CSV files into R. 

```{r cache=TRUE}
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "training.csv", method="curl")
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "testing.csv", method="curl")
trainingCsv<-read.csv("training.csv")
testingCsv<-read.csv("testing.csv")
```
Now we want to remove columns with data, which is irrelevant for the prediection. The rows having a column "new_window" = "yes" are special rows which summarize data within some time window. We are not asked to predict for such time windows, so such rows must not be used for training. We remove all these rows from the dataset. Further more, we remove all columns which have all NA values, since they do not represent any information gain at all. Also we remove columns containing timestamp, time windows number and row number for the same reason.

```{r cache=TRUE}
noNewWindowTraining<-trainingCsv[trainingCsv$new_window == "no",]
#list of columns in the training dataset
columns<-as.list(noNewWindowTraining)
#find columns in the training dataset containing only NAs
naColumns<-which(sapply(columns, function(c) all(is.na(c) | c=='')))
naColumns<-as.vector(naColumns)

removeIrrelevantColumns<-function (d,naColumns){
        return(d[-c(1,3:7, naColumns)])
}
trainingAll<-removeIrrelevantColumns(noNewWindowTraining , naColumns)
resultTesting<-removeIrrelevantColumns(testingCsv, naColumns)
```
We build a tesing and traing dataset. 60% of the data is used for training.

```{r cache=TRUE}
library("caret")
set.seed(13323)
inTrain<-createDataPartition(y<-trainingAll$classe, p=0.60, list=FALSE)
training<-trainingAll[inTrain, ]
testing<-trainingAll[-inTrain,]
```
For machine learning we choose random forest algorithm, which is simple to implement and performs good for classification problems with multiple predictors. To improve the computation speed we use only 2 resampling iterations, but stil produce acceptable results. Note that we parallelize computations (on multicore hardware) with the help of doParallel library.

```{r cache=TRUE}
ctrl<-trainControl(number=2)
library(parallel); library(doParallel)
registerDoParallel(clust <- makeForkCluster(detectCores()))

fit<-train(data = training, classe ~ . , method='rf', trControl = ctrl)
stopCluster(clust)
```
See the confusion matrix for the test data below. The cross validations estimates the accuracy on the out of sample data to be more than 99%, which is acceptable for us.

```{r cache=TRUE}
predicted<-predict(fit, newdata=testing)
confusionMatrix(predicted, testing$classe)
```